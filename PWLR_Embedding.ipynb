{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class GraphDataset\n",
    "    - Reads a collection of graphs stored in \".gml\" file format.\n",
    "    - Reads a list of node labels\n",
    "    - Computes maximum node degrees, maximum # nodes, and maximum # cycles\n",
    "    - Reads a list of tuples of unweighted node degrees connected by edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import glob\n",
    "\n",
    "class GraphDataset:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.dataset = sorted(glob.glob(\"data/\"+name+\"/*.gml\"))\n",
    "        self.label_list = []\n",
    "        self.max_degree = 0\n",
    "        self.max_vertex = 0\n",
    "        self.max_cycle = 0\n",
    "    \n",
    "    def get_node_label(self):\n",
    "        #computes label_list\n",
    "        \n",
    "        label_set = set()\n",
    "        for file in self.dataset:\n",
    "            g = ig.read(file)\n",
    "            label_local = set(g.vs[\"label\"])\n",
    "            for item in label_local:\n",
    "                label_set.add(item)\n",
    "        \n",
    "        check_column=[]\n",
    "        for check in g.vs.attributes():\n",
    "             if check != 'id' and check != 'label':\n",
    "                check_column+=[check]           \n",
    "        if check_column:        \n",
    "            total_mat=[]\n",
    "            for file in self.dataset:\n",
    "                g = ig.read(file) \n",
    "                label_mat=[]\n",
    "                for node in g.vs:\n",
    "                    labels=[]\n",
    "                    for column in (check_column):\n",
    "                        labels+=[float(node[column])]\n",
    "                    label_mat.append(labels)\n",
    "                total_mat.append(np.array(label_mat))\n",
    "            mm_check=np.concatenate(total_mat,0)\n",
    "            self.label_min=np.min(mm_check,axis=0)\n",
    "            self.label_max=np.max(mm_check,axis=0)\n",
    "            self.label_mean=np.mean(mm_check,axis=0)\n",
    "            self.label_std=np.std(mm_check,axis=0)\n",
    "            \n",
    "        label_mid = [(entry,chr(int(entry)+32)) for entry in label_set]\n",
    "        label_mid.sort(key=lambda x:x[1])\n",
    "        self.label_list = [entry[0] for entry in label_mid]\n",
    "    \n",
    "    def get_max_degree(self):\n",
    "        #computes max_degree\n",
    "        \n",
    "        max_deg = 0\n",
    "        for file in self.dataset:\n",
    "            g = ig.read(file)\n",
    "            local_deg = g.maxdegree()\n",
    "            if local_deg > max_deg:\n",
    "                max_deg = local_deg\n",
    "        self.max_degree = max_deg\n",
    "    \n",
    "    def get_max_vertex(self):\n",
    "        #computes max_vertex\n",
    "        \n",
    "        max_node = 0\n",
    "        for file in self.dataset:\n",
    "            g = ig.read(file)\n",
    "            local_node = g.vcount()\n",
    "            if local_node > max_node:\n",
    "                max_node = local_node\n",
    "        self.max_vertex = max_node\n",
    "    \n",
    "    def get_max_cycle(self):\n",
    "        #computes max_cycle\n",
    "        \n",
    "        max_cyc = 0\n",
    "        for file in self.dataset:\n",
    "            g = ig.read(file)\n",
    "            local_cyc = g.ecount() - g.vcount() + len(g.components())\n",
    "            if local_cyc > max_cyc:\n",
    "                max_cyc = local_cyc\n",
    "        self.max_cycle = max_cyc\n",
    "        \n",
    "    def get_edge_degree_list(self):\n",
    "        #computes a list of tuples of unweighted node degrees\n",
    "        \n",
    "        edge_list = []\n",
    "        for file in self.dataset:\n",
    "            g = ig.read(file)\n",
    "            for e in g.es:\n",
    "                v1,v2 = e.source, e.target\n",
    "                e_deg = sorted(((g.vs[v1]).degree(),(g.vs[v2]).degree()))\n",
    "                if e_deg not in edge_list:\n",
    "                    edge_list += [e_deg]\n",
    "        \n",
    "        edge_list = sorted(edge_list)\n",
    "        return edge_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class PWLR\n",
    "    - Embeds a dataset of graphs to R^n using random walk (Markov chains) and persistent homology\n",
    "    - Produces both PWLR-H_i and PWLR-OPT-H_i representations\n",
    "    - Exports the embedded vectors in \".npy\" format in \"embed\" folder\n",
    "    - Dimensions:\n",
    "        + 1st dimension : graph dataset number\n",
    "        + 2nd dimension : WL iteration steps\n",
    "        + 3rd dimension : RW iteration steps\n",
    "        + 4th dimension : real embedded vector components\n",
    "        Ex) A[0][2][5]: A real embedding of graph no.0 obtained from applying WL 2 times & RW 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import igraph as ig\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class PWLR:\n",
    "    def __init__(self, name, label_type, output_type='single'):\n",
    "        \n",
    "        self.name = name\n",
    "        self.dataset = sorted(glob.glob(\"data/\"+name+\"/*.gml\"))\n",
    "        self.dataset_type = label_type\n",
    "        self.output_type=output_type\n",
    "        graph_data = GraphDataset(self.name)\n",
    "        graph_data.get_max_degree()\n",
    "        graph_data.get_max_vertex()\n",
    "        graph_data.get_max_cycle()\n",
    "        \n",
    "        if (self.dataset_type == 'Discrete'):\n",
    "            graph_data.get_node_label()\n",
    "            self.label_dict = graph_data.label_list \n",
    "        elif (self.dataset_type == 'Continuous'):\n",
    "            graph_data.get_node_label()\n",
    "            self.label_dict = graph_data.label_list\n",
    "            self.label_min=graph_data.label_min   \n",
    "            self.label_max=graph_data.label_max\n",
    "            self.label_mean=graph_data.label_mean\n",
    "            self.label_std=graph_data.label_std\n",
    "        else:\n",
    "            self.label_dict = ['1']\n",
    "        \n",
    "        self.max_degree = graph_data.max_degree #maximum degree of nodes of graphs in the dataset\n",
    "        self.max_vertex = graph_data.max_vertex #maximum number of nodes of graphs in the dataset\n",
    "        self.max_cycle = graph_data.max_cycle # maximum number of cycles of graphs in the dataset\n",
    "        self.data_edge_list = graph_data.get_edge_degree_list() #list of edge degrees\n",
    "        \n",
    "        self.homology_zeroth = pd.DataFrame([]) #H0 (component) PWLR embedding\n",
    "        self.homology_first = pd.DataFrame([]) #H1 (cycle) PWLR embedding\n",
    "        \n",
    "        #Variable for each graph\n",
    "        self.graph = 0 #read a graph using igraph python library\n",
    "        self.graph_node_num = 0 #number of nodes of a graph\n",
    "        self.graph_label_matrix = torch.tensor([]) #node label matrix of a graph\n",
    "        self.graph_label_matrix_init = torch.tensor([]) #initial node label matrix of a graph\n",
    "        self.graph_adj_matrix = torch.tensor([]) #adjacency matrix of a graph\n",
    "        self.graph_norm_adj_matrix = torch.tensor([]) #normalized adjacency matrix of a graph with added self-loops\n",
    "        self.graph_weight_matrix = torch.tensor([]) #adjacency matrix with edge weights obtained from the L_p distance between adjacent node labels\n",
    "        self.graph_weight_list = torch.tensor([]) #a list of edge weights obtained from L_p distances between any two adjacent node labels\n",
    "        self.graph_homology_zeroth = torch.tensor([]) #H0 (component) PWLR embedding\n",
    "        self.graph_homology_first = torch.tensor([]) #H1 (cycle) PWLR embedding\n",
    "    \n",
    "    def read_edge_degree_list(self):\n",
    "        graph_data = GraphDataset(self.name)\n",
    "        self.data_edge_list = graph_data.get_edge_degree_list() #list of edge degrees\n",
    "    \n",
    "    #Compile a single graph from a dataset\n",
    "    def read_graph(self, graph_name):\n",
    "        self.graph = ig.read(graph_name)\n",
    "        self.graph_node_num = (self.graph).vcount()\n",
    "        self.graph_label_matrix = []\n",
    "    \n",
    "    #Given a compiled graph, construct the normalized adjacency matrix\n",
    "    def graph_get_norm_adjacency(self, weight=True):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        \n",
    "        if weight:\n",
    "            weight_attr = \"weight\"\n",
    "        elif weight == \"attribute\":\n",
    "            weight_attr = \"attribute0\"\n",
    "        else:\n",
    "            weight_attr = None\n",
    "            \n",
    "        adj_matrix = list((self.graph).get_adjacency(attribute = weight_attr))\n",
    "        self.graph_adj_matrix = torch.tensor(adj_matrix)\n",
    "\n",
    "        adj_matrix_self_loop = self.graph_adj_matrix + torch.eye(self.graph_node_num)\n",
    "        self.graph_norm_adj_matrix = torch.nn.functional.normalize(adj_matrix_self_loop,p=1.0)\n",
    "    \n",
    "    #Given a compiled graph, construct a matrix of node labels. (Discrete Labels)\n",
    "    def graph_get_init_node_label(self):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        \n",
    "        label_mat1 = []\n",
    "        label_num = []\n",
    "        \n",
    "        for label_ref in self.label_dict:\n",
    "            label_vector = []\n",
    "            label_sub = 0\n",
    "            for node in (self.graph).vs:\n",
    "                if self.dataset_type == 'Discrete' or self.output_type == 'both':\n",
    "                    if node[\"label\"] == label_ref:\n",
    "                        label_vector += [1]\n",
    "                        label_sub += 1\n",
    "                    else:\n",
    "                        label_vector += [0]\n",
    "                elif self.dataset_type == 'No Label':\n",
    "                    label_vector += [1]\n",
    "                    label_sub += 1\n",
    "                else:\n",
    "                    label_vector += [0]\n",
    "                    label_sub += 1\n",
    "            label_mat1 += [label_vector]\n",
    "            label_num += [label_sub]\n",
    "        label_mat_a = np.array(label_mat1)\n",
    "        \n",
    "        label_mat2=[]\n",
    "        label_mat_b=[]\n",
    "        check_column=[]\n",
    "        \n",
    "        if self.output_type =='single':\n",
    "            for check in (self.graph).vs.attributes():\n",
    "                if check != 'id' and check != 'label':\n",
    "                    check_column+=[check] \n",
    "            if check_column: \n",
    "                for node in (self.graph).vs:\n",
    "                    labels=[]\n",
    "                    for j, column in enumerate(check_column):\n",
    "                        labels+=[float(node[column])]\n",
    "                    label_mat2.append(labels)           \n",
    "                label_mat_b=np.transpose(label_mat2,(1,0))\n",
    "        else : \n",
    "            for check in (self.graph).vs.attributes():\n",
    "                if check != 'id' and check != 'label':\n",
    "                    check_column+=[check] \n",
    "            if check_column: \n",
    "                for node in (self.graph).vs:\n",
    "                    labels=[]\n",
    "                    for j, column in enumerate(check_column):\n",
    "                        labels+=[float(node[column])]\n",
    "                    label_mat2.append(labels)           \n",
    "                label_mat_b=np.transpose(label_mat2,(1,0))\n",
    "        \n",
    "        if self.output_type == 'single':\n",
    "            if self.dataset_type == 'Discrete':\n",
    "                label_mat=label_mat_a\n",
    "            elif self.dataset_type == 'Continuous':\n",
    "                label_mat=label_mat_b\n",
    "        elif self.output_type == 'both':\n",
    "            label_mat=np.concatenate((label_mat_a, label_mat_b), axis=0)\n",
    "        \n",
    "        label_mat=torch.FloatTensor(label_mat)\n",
    "        self.graph_label_matrix = label_mat\n",
    "        self.graph_label_matrix_init = label_mat\n",
    "    \n",
    "    #Update node labels once using Markov chains (Random Walks)\n",
    "    def graph_label_update(self):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        \n",
    "        rw_mat = self.graph_norm_adj_matrix\n",
    "        self.graph_label_matrix = torch.matmul(self.graph_label_matrix, rw_mat)    \n",
    "                \n",
    "    def graph_label_update_WL(self):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        \n",
    "        rw_mat = self.graph_norm_adj_matrix\n",
    "        self.graph_label_matrix = torch.transpose(torch.matmul(rw_mat, torch.transpose(self.graph_label_matrix,0,1)),0,1)\n",
    "    \n",
    "    #Compute the adjacency matrix with weights on edges obtained from the L_p distance between adjacent node labels\n",
    "    def graph_edge_weight_matrix(self, p=1):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        \n",
    "        edge_weight = []\n",
    "        edge_list = torch.nonzero((self.graph_adj_matrix).gt(0))\n",
    "        graph_label = torch.transpose(self.graph_label_matrix,0,1)\n",
    "        graph_node = self.graph_node_num\n",
    "        for edge in edge_list:\n",
    "            node_label_diff = graph_label[edge[0]] - graph_label[edge[1]]\n",
    "            weight = torch.norm(node_label_diff,p) + 1 ##Add a Bias Term +1\n",
    "            edge_weight += [weight]\n",
    "    \n",
    "        edge_list_and_weight = torch.sparse_coo_tensor(edge_list.t(), edge_weight, (graph_node, graph_node))\n",
    "        self.graph_weight_matrix = edge_list_and_weight.to_dense()\n",
    "        self.graph_weight_list = torch.unique(torch.tensor(sorted(edge_weight)))\n",
    "\n",
    "    def graph_unsorted_weight_list(self,p=1):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        edge_weight = []\n",
    "        old_edge_list = torch.nonzero((self.graph_adj_matrix).gt(0))\n",
    "        graph_label = torch.transpose(self.graph_label_matrix,0,1)\n",
    "        graph_node = self.graph_node_num\n",
    "        \n",
    "        edge_list = []\n",
    "        for edge in old_edge_list:\n",
    "            edge = sorted(edge.tolist())\n",
    "            if edge not in edge_list:\n",
    "                edge_list += [edge]\n",
    "        \n",
    "        for edge in edge_list:\n",
    "            node_label_diff = graph_label[edge[0]] - graph_label[edge[1]]\n",
    "            weight = torch.norm(node_label_diff,p) + 1 ##Add a Bias Term +1\n",
    "            edge_weight += [weight]\n",
    "        return torch.tensor(edge_weight), torch.tensor(edge_list)\n",
    "    \n",
    "    #Compute PWLR-H0 and PWLR-H1 for each graph.\n",
    "    def graph_label_persistence(self, error=1e-12,p=1):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        \n",
    "        edge_degree_list = self.data_edge_list\n",
    "        unsorted_edge_weight, unsorted_edge_list = self.graph_unsorted_weight_list(p)\n",
    "        homology_zeroth = []\n",
    "        homology_first = []\n",
    "        \n",
    "        component = self.graph_node_num\n",
    "        cycle = 0\n",
    "        \n",
    "        #initialize graph\n",
    "        h = ig.Graph()\n",
    "        h.add_vertices((self.graph).vcount())\n",
    "        for weight in self.graph_weight_list:\n",
    "            #Obtain the corresponding edge with the exact weight from the original graph\n",
    "            masked_exact_edge_index = (unsorted_edge_weight == weight).nonzero(as_tuple=False)\n",
    "            masked_exact_edge_index = masked_exact_edge_index.tolist()\n",
    "            \n",
    "            masked_exact_edge = []\n",
    "            for local_ind in masked_exact_edge_index:\n",
    "                new_local_ind = (unsorted_edge_list[local_ind]).tolist()\n",
    "                new_local_ind = [int(x) for x in new_local_ind[0]]\n",
    "                masked_exact_edge += [new_local_ind]\n",
    "            \n",
    "            #Add Edges\n",
    "            h.add_edges(masked_exact_edge)\n",
    "            h_component = len(h.components())\n",
    "            h_node = h.vcount()\n",
    "            h_edge = h.ecount()\n",
    "            h_cycle = h_edge - h_node + h_component\n",
    "                \n",
    "            component_diff = component - h_component\n",
    "            cycle_diff = h_cycle - cycle\n",
    "            new_weight = weight #May subtract bias terms if needed\n",
    "                \n",
    "            if component_diff > 0:\n",
    "                component = h_component\n",
    "                homology_zeroth += [float(new_weight)]*component_diff \n",
    "            if cycle_diff > 0:\n",
    "                cycle = h_cycle\n",
    "                homology_first += [float(new_weight)]*cycle_diff\n",
    "                \n",
    "        self.graph_homology_zeroth = torch.tensor(homology_zeroth)\n",
    "        self.graph_homology_first = torch.tensor(homology_first)\n",
    "    \n",
    "    #pad the embedded graphs\n",
    "    def graph_embed_pad(self, bias=float(\"nan\")):\n",
    "        h0_dim = len(self.graph_homology_zeroth)\n",
    "        h1_dim = len(self.graph_homology_first)\n",
    "        h0_max = self.max_vertex\n",
    "        h1_max = self.max_cycle\n",
    "\n",
    "        self.graph_homology_zeroth = torch.cat((self.graph_homology_zeroth, torch.tensor([bias]*(h0_max - h0_dim))),0)\n",
    "        self.graph_homology_first = torch.cat((self.graph_homology_first, torch.tensor([bias]*(h1_max - h1_dim))),0)\n",
    "    \n",
    "    def edge_list_degree(self,input_edge_list):\n",
    "        output_edge_list = []\n",
    "        for input_edge in input_edge_list:\n",
    "            input_source = (self.graph).vs[input_edge[0]]\n",
    "            input_target = (self.graph).vs[input_edge[1]]\n",
    "            output_deg = sorted([input_source.degree(), input_target.degree()])\n",
    "            output_edge_list += [output_deg]\n",
    "        return output_edge_list\n",
    "    \n",
    "    #Compute PWLR-OPT-H0 and PWLR-OPT-H1 for each graph.\n",
    "    def graph_label_persistence_node_degree(self,error=1e-12,p=1):\n",
    "        if self.graph == 0:\n",
    "            return \"Graph Compilation Required\"\n",
    "        \n",
    "        edge_degree_list = self.data_edge_list\n",
    "        unsorted_edge_weight, unsorted_edge_list = self.graph_unsorted_weight_list(p)\n",
    "        homology_zeroth = [0]*len(edge_degree_list)\n",
    "        homology_first = [0]*len(edge_degree_list)\n",
    "        \n",
    "        component = self.graph_node_num\n",
    "        cycle = 0\n",
    "        \n",
    "        #initialize graph\n",
    "        h = ig.Graph()\n",
    "        h.add_vertices((self.graph).vcount())\n",
    "        for weight in self.graph_weight_list:\n",
    "            #Obtain the corresponding edge with the exact weight from the original graph\n",
    "            masked_exact_edge_index = (unsorted_edge_weight == weight).nonzero(as_tuple=False)\n",
    "            masked_exact_edge_index = masked_exact_edge_index.tolist()\n",
    "            \n",
    "            masked_exact_edge = []\n",
    "            for local_ind in masked_exact_edge_index:\n",
    "                new_local_ind = (unsorted_edge_list[local_ind]).tolist()\n",
    "                new_local_ind = [int(x) for x in new_local_ind[0]]\n",
    "                masked_exact_edge += [new_local_ind]\n",
    "            \n",
    "            #obtain edge degrees\n",
    "            masked_exact_edge_degree = self.edge_list_degree(masked_exact_edge)\n",
    "            \n",
    "            #Add Edges\n",
    "            for i in range(0,len(masked_exact_edge)):\n",
    "                edge_exact = masked_exact_edge[i]\n",
    "                edge_exact_deg = masked_exact_edge_degree[i]\n",
    "                edge_exact_ind = edge_degree_list.index(edge_exact_deg)\n",
    "                \n",
    "                h.add_edges([edge_exact])\n",
    "                h_component = len(h.components())\n",
    "                h_node = h.vcount()\n",
    "                h_edge = h.ecount()\n",
    "                h_cycle = h_edge - h_node + h_component\n",
    "                \n",
    "                component_diff = component - h_component\n",
    "                cycle_diff = h_cycle - cycle\n",
    "                new_weight = weight #May subtract bias terms if needed\n",
    "                \n",
    "                if component_diff > 0:\n",
    "                    component = h_component\n",
    "                    homology_zeroth[edge_exact_ind] += new_weight**p\n",
    "                if cycle_diff > 0:\n",
    "                    cycle = h_cycle\n",
    "                    homology_first[edge_exact_ind] += new_weight**p\n",
    "        \n",
    "        self.graph_homology_zeroth = torch.tensor(homology_zeroth)\n",
    "        self.graph_homology_first = torch.tensor(homology_first)\n",
    "\n",
    "    def embed_graph_exact_dataset(self, weight=True, p=1, error=1e-12, bias=-1.0, markov_step=1, wl_step=1, embed_type=1):\n",
    "        total_h0_data = []\n",
    "        total_h1_data = []\n",
    "        graph_idx=0\n",
    "        for files in self.dataset:\n",
    "            h0_data = []\n",
    "            h1_data = []\n",
    "            wl_h0_data = []\n",
    "            wl_h1_data = []\n",
    "\n",
    "            self.read_graph(files)\n",
    "            self.graph_get_norm_adjacency(weight)\n",
    "            self.graph_get_init_node_label()\n",
    "            self.graph_label_matrix = self.graph_label_matrix_init\n",
    "            for k_index in range(0,wl_step):\n",
    "                self.graph_label_update_WL()\n",
    "            for k_index in range(0,markov_step):\n",
    "                self.graph_label_update()\n",
    "\n",
    "            if embed_type == 1:\n",
    "                self.graph_edge_weight_matrix(p)\n",
    "                self.graph_label_persistence(error)\n",
    "                self.graph_embed_pad(bias)\n",
    "            else:\n",
    "                embed_type = 2\n",
    "                self.graph_edge_weight_matrix(p)\n",
    "                self.graph_label_persistence_node_degree(error,p)\n",
    "\n",
    "            h0_graph = list(self.graph_homology_zeroth)\n",
    "            h1_graph = list(self.graph_homology_first)\n",
    "            wl_h0_data += [h0_graph]\n",
    "            wl_h1_data += [h1_graph]\n",
    "            h0_data += [wl_h0_data]\n",
    "            h1_data += [wl_h1_data]\n",
    "            total_h0_data += [h0_data]\n",
    "            total_h1_data += [h1_data]\n",
    "\n",
    "        total_h0_data = np.array(total_h0_data)\n",
    "        total_h1_data = np.array(total_h1_data)\n",
    "        return total_h0_data, total_h1_data\n",
    "    \n",
    "    #embed_type = 1: Export embedded vectors of form PWLR-H_i for all graphs in a given dataset as .npy file\n",
    "    #embed_type = 2: Export embedded vectors of form PWLR-OPT-H_i for all graphs in a given dataset as .npy file\n",
    "    def embed_graph_dataset(self, weight=True, p=1, error=1e-12, bias=-1.0, markov_step=1, wl_step=1, embed_type=1):\n",
    "        total_h0_data = []\n",
    "        total_h1_data = []\n",
    "        graph_idx=0\n",
    "        for files in self.dataset: \n",
    "            graph_idx += 1\n",
    "            if graph_idx % 100 == 0:\n",
    "                print(graph_idx)\n",
    "            h0_data = []\n",
    "            h1_data = []\n",
    "            \n",
    "            #Embed the graph\n",
    "            self.read_graph(files)\n",
    "            self.graph_get_norm_adjacency(weight)\n",
    "            self.graph_get_init_node_label()\n",
    "            graph_label_only_wl=[]\n",
    "            self.graph_label_matrix = self.graph_label_matrix_init\n",
    "            graph_label_only_wl.append(self.graph_label_matrix)            \n",
    "            \n",
    "            for _ in range(1,wl_step):\n",
    "                self.graph_label_update_WL()\n",
    "                graph_label_only_wl.append(self.graph_label_matrix)\n",
    "\n",
    "            for step_wl in range(0, wl_step):  \n",
    "                wl_h0_data = []\n",
    "                wl_h1_data = []\n",
    "\n",
    "                self.graph_label_matrix=graph_label_only_wl[step_wl] \n",
    "                for step in range(0,markov_step):\n",
    "                    if embed_type == 1:\n",
    "                        self.graph_edge_weight_matrix(p)\n",
    "                        self.graph_label_persistence(error)\n",
    "                        self.graph_embed_pad(bias)\n",
    "                    else:\n",
    "                        embed_type = 2\n",
    "                        self.graph_edge_weight_matrix(p)\n",
    "                        self.graph_label_persistence_node_degree(error,p)\n",
    "\n",
    "                    h0_graph = list(self.graph_homology_zeroth)\n",
    "                    h1_graph = list(self.graph_homology_first)\n",
    "                    wl_h0_data += [h0_graph]\n",
    "                    wl_h1_data += [h1_graph]\n",
    "                    self.graph_label_update()\n",
    "\n",
    "                h0_data += [wl_h0_data]\n",
    "                h1_data += [wl_h1_data]  \n",
    "                \n",
    "            total_h0_data += [h0_data]\n",
    "            total_h1_data += [h1_data]                       \n",
    "\n",
    "\n",
    "        total_h0_data = np.array(total_h0_data)\n",
    "        total_h1_data = np.array(total_h1_data)                \n",
    "       \n",
    "        if self.dataset_type == 'Discrete':\n",
    "            np.save(\"embed/\"+self.name+f\"_discrete_component_seq_{wl_step}_{markov_step}_ver{embed_type}\", total_h0_data)\n",
    "            np.save(\"embed/\"+self.name+f\"_discrete_cycle_seq_{wl_step}_{markov_step}_ver{embed_type}\", total_h1_data)\n",
    "        elif self.dataset_type == 'Continuous':\n",
    "            np.save(\"embed/\"+self.name+f\"_component_seq_{wl_step}_{markov_step}_ver{embed_type}\", total_h0_data)\n",
    "            np.save(\"embed/\"+self.name+f\"_cycle_seq_{wl_step}_{markov_step}_ver{embed_type}\", total_h1_data)\n",
    "        else:\n",
    "            np.save(\"embed/\"+self.name+f\"_no_label_component_seq_{wl_step}_{markov_step}_ver{embed_type}\", total_h0_data)\n",
    "            np.save(\"embed/\"+self.name+f\"_no_label_cycle_seq_{wl_step}_{markov_step}_ver{embed_type}\", total_h1_data)\n",
    "    \n",
    "    #embed_type=1: Export embedded vectors of form PWLR-H_i for a single graph as .npy file\n",
    "    #embed_type=2: Export embedded vectors of form PWLR-OPT-H_i for a single graph as .npy file\n",
    "    #Allows parallel Computing\n",
    "    def embed_graph(self, file_name, weight=True, p=1, error=1e-12, bias=-1.0, markov_step=1, wl_step=1, embed_type=1):\n",
    "        h0_data = []\n",
    "        h1_data = []\n",
    "        \n",
    "        #Embed the graph\n",
    "        self.read_graph(file_name)\n",
    "        self.graph_get_norm_adjacency(weight)\n",
    "        self.graph_get_init_node_label()\n",
    "        graph_label_only_wl=[]\n",
    "        self.graph_label_matrix = self.graph_label_matrix_init\n",
    "        graph_label_only_wl.append(self.graph_label_matrix)            \n",
    "                       \n",
    "        for _ in range(1,wl_step):\n",
    "            self.graph_label_update_WL()\n",
    "            graph_label_only_wl.append(self.graph_label_matrix)\n",
    "\n",
    "        for step_wl in range(0, wl_step):  \n",
    "            wl_h0_data = []\n",
    "            wl_h1_data = []\n",
    "\n",
    "            self.graph_label_matrix=graph_label_only_wl[step_wl] \n",
    "            for step in range(0,markov_step):\n",
    "                if embed_type == 1:\n",
    "                    self.graph_edge_weight_matrix(p)\n",
    "                    self.graph_label_persistence(error)\n",
    "                    self.graph_embed_pad(bias)\n",
    "                else:\n",
    "                    embed_type=2\n",
    "                    self.graph_edge_weight_matrix(p)\n",
    "                    self.graph_label_persistence_node_degree(error,p)\n",
    "\n",
    "                h0_graph = list(self.graph_homology_zeroth)\n",
    "                h1_graph = list(self.graph_homology_first)\n",
    "                wl_h0_data += [h0_graph]\n",
    "                wl_h1_data += [h1_graph]\n",
    "                self.graph_label_update()\n",
    "\n",
    "            h0_data += [wl_h0_data]\n",
    "            h1_data += [wl_h1_data]  \n",
    "        \n",
    "        number_length = len(list(filter(str.isdigit, file_name)))\n",
    "        if self.dataset_type == 'Discrete':\n",
    "            np.save(\"embed/\"+self.name+\"/\"+file_name[-4-number_length:-4]+\"_discrete_component_seq_\"+str(embed_type), h0_data)\n",
    "            np.save(\"embed/\"+self.name+\"/\"+file_name[-4-number_length:-4]+\"_discrete_cycle_seq_\"+str(embed_type), h1_data)\n",
    "        elif self.dataset_type == 'Continuous':\n",
    "            np.save(\"embed/\"+self.name+\"/\"+file_name[-4-number_length:-4]+\"_component_seq_\"+str(embed_type), h0_data)\n",
    "            np.save(\"embed/\"+self.name+\"/\"+file_name[-4-number_length:-4]+\"_cycle_seq_\"+str(embed_type), h1_data)\n",
    "        else:\n",
    "            np.save(\"embed/\"+self.name+\"/\"+file_name[-4-number_length:-4]+\"_no_label_component_seq_\"+str(embed_type), h0_data)\n",
    "            np.save(\"embed/\"+self.name+\"/\"+file_name[-4-number_length:-4]+\"_no_label_cycle_seq_\"+str(embed_type), h1_data)\n",
    "        \n",
    "    def concatenator(self, embed_type=1):\n",
    "        if embed_type != 1:\n",
    "            embed_type = 2\n",
    "        \n",
    "        #load each graph representation\n",
    "        if self.dataset_type == 'Discrete':\n",
    "            h0_dataset_file = sorted(glob.glob(\"embed/\"+self.name+\"/*_discrete_component_seq_\"+str(embed_type)+\".npy\"))\n",
    "            h1_dataset_file = sorted(glob.glob(\"embed/\"+self.name+\"/*_discrete_cycle_seq_\"+str(embed_type)+\".npy\"))\n",
    "        elif self.dataset_type == 'Continuous':\n",
    "            h0_dataset_file = sorted(glob.glob(\"embed/\"+self.name+\"/*_component_seq_\"+str(embed_type)+\".npy\"))\n",
    "            h1_dataset_file = sorted(glob.glob(\"embed/\"+self.name+\"/*_cycle_seq_\"+str(embed_type)+\".npy\"))\n",
    "            h0_dataset_file = list(filter(lambda x: not re.search('discrete', x), h0_dataset_file))\n",
    "            h1_dataset_file = list(filter(lambda x: not re.search('discrete', x), h1_dataset_file))\n",
    "        else:\n",
    "            h0_dataset_file = sorted(glob.glob(\"embed/\"+self.name+\"/*_no_label_component_seq_\"+str(embed_type)+\".npy\"))\n",
    "            h1_dataset_file = sorted(glob.glob(\"embed/\"+self.name+\"/*_no_label_cycle_seq_\"+str(embed_type)+\".npy\"))\n",
    "        sample_file_0 = np.load(h0_dataset_file[0])\n",
    "        sample_file_1 = np.load(h1_dataset_file[0])\n",
    "\n",
    "        sample_file_0_size = sample_file_0.shape\n",
    "        sample_file_1_size = sample_file_1.shape\n",
    "        h0_dataset_array = np.empty(shape=(0,*sample_file_0_size))\n",
    "        h1_dataset_array = np.empty(shape=(0,*sample_file_1_size))\n",
    "        \n",
    "        #Concatenate Files\n",
    "        i = 0\n",
    "        for files in h0_dataset_file:\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            h0_dataset_array = np.append(h0_dataset_array, [np.load(files)], axis=0)\n",
    "        i = 0\n",
    "        for files in h1_dataset_file:\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            h1_dataset_array = np.append(h1_dataset_array, [np.load(files)], axis=0)\n",
    "        \n",
    "        #Export Files\n",
    "        if self.dataset_type == 'Discrete':\n",
    "            np.save(\"embed/\"+self.name+f\"_discrete_component_seq_{sample_file_0_size[0]}_{sample_file_0_size[1]}_ver{embed_type}\", h0_dataset_array)\n",
    "            np.save(\"embed/\"+self.name+f\"_discrete_cycle_seq_{sample_file_1_size[0]}_{sample_file_1_size[1]}_ver{embed_type}\", h1_dataset_array)\n",
    "        elif self.dataset_type == 'Continuous':\n",
    "            np.save(\"embed/\"+self.name+f\"_component_seq_{sample_file_0_size[0]}_{sample_file_0_size[1]}_ver{embed_type}\", h0_dataset_array)\n",
    "            np.save(\"embed/\"+self.name+f\"_cycle_seq_{sample_file_1_size[0]}_{sample_file_1_size[1]}_ver{embed_type}\", h1_dataset_array)\n",
    "        else:\n",
    "            np.save(\"embed/\"+self.name+f\"_no_label_component_seq_{sample_file_0_size[0]}_{sample_file_0_size[1]}_ver{embed_type}\", h0_dataset_array)\n",
    "            np.save(\"embed/\"+self.name+f\"_no_label_cycle_seq_{sample_file_1_size[0]}_{sample_file_1_size[1]}_ver{embed_type}\", h1_dataset_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUTAG & PTC_FR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   48.8s\n",
      "[Parallel(n_jobs=8)]: Done 188 out of 188 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   50.7s\n",
      "[Parallel(n_jobs=8)]: Done 188 out of 188 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "PTC_FR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   43.1s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done 351 out of 351 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=8)]: Done 351 out of 351 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "dataset_name_list = [\"MUTAG\", \"PTC_FR\"]\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    print(dataset_name)\n",
    "    label_dataset_type = \"Discrete\"\n",
    "    OUTPUT_TYPE='single'\n",
    "    data = PWLR(dataset_name, label_dataset_type, output_type=OUTPUT_TYPE)\n",
    "    \n",
    "    n_jobs=8\n",
    "    file_dataset = sorted(glob.glob(\"data/\"+dataset_name+\"/*.gml\"))\n",
    "    \n",
    "    #PWLR-H0 and PWLR-H1 representations\n",
    "    Parallel(n_jobs,verbose=5)(delayed(data.embed_graph)(file_name, weight=True, p=1, error=1e-12, bias=0, markov_step=30, wl_step=30, embed_type=1) for file_name in file_dataset)\n",
    "    data.concatenator(embed_type=1)\n",
    "    \n",
    "    #PWLR-OPT-H0 and PWLR-OPT-H1 representations\n",
    "    Parallel(n_jobs,verbose=5)(delayed(data.embed_graph)(file_name, weight=True, p=1, error=1e-12, bias=0, markov_step=30, wl_step=30, embed_type=2) for file_name in file_dataset)\n",
    "    data.concatenator(embed_type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BZR & BZR-MD datasets (Discrete labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BZR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=8)]: Done 405 out of 405 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=8)]: Done 405 out of 405 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "BZR_MD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=8)]: Done 306 out of 306 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   56.4s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=8)]: Done 306 out of 306 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "dataset_name_list = [\"BZR\", \"BZR_MD\"]\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    print(dataset_name)\n",
    "    label_dataset_type = \"Discrete\"\n",
    "    OUTPUT_TYPE='single'\n",
    "    data = PWLR(dataset_name, label_dataset_type, output_type=OUTPUT_TYPE)\n",
    "    \n",
    "    n_jobs=8\n",
    "    file_dataset = sorted(glob.glob(\"data/\"+dataset_name+\"/*.gml\"))\n",
    "    \n",
    "    #PWLR-H0 and PWLR-H1 representations\n",
    "    Parallel(n_jobs,verbose=5)(delayed(data.embed_graph)(file_name, weight=\"attribute\", p=1, error=1e-12, bias=0, markov_step=30, wl_step=30, embed_type=1) for file_name in file_dataset)\n",
    "    data.concatenator(embed_type=1)\n",
    "    \n",
    "    #PWLR-OPT-H0 and PWLR-OPT-H1 representations\n",
    "    Parallel(n_jobs,verbose=5)(delayed(data.embed_graph)(file_name, weight=\"attribute\", p=1, error=1e-12, bias=0, markov_step=30, wl_step=30, embed_type=2) for file_name in file_dataset)\n",
    "    data.concatenator(embed_type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BZR dataset (Discrete & Continuous labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BZR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=8)]: Done 405 out of 405 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 405 out of 405 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "dataset_name_list = [\"BZR\"]\n",
    "\n",
    "for dataset_name in dataset_name_list:\n",
    "    print(dataset_name)\n",
    "    label_dataset_type = \"Continuous\"\n",
    "    OUTPUT_TYPE='both'\n",
    "    data = PWLR(dataset_name, label_dataset_type, output_type=OUTPUT_TYPE)\n",
    "    \n",
    "    n_jobs=8\n",
    "    file_dataset = sorted(glob.glob(\"data/\"+dataset_name+\"/*.gml\"))\n",
    "    \n",
    "    #PWLR-H0 and PWLR-H1 representations\n",
    "    Parallel(n_jobs,verbose=5)(delayed(data.embed_graph)(file_name, weight=\"attribute\", p=1, error=1e-12, bias=0, markov_step=30, wl_step=30, embed_type=1) for file_name in file_dataset)\n",
    "    data.concatenator(embed_type=1)\n",
    "    \n",
    "    #PWLR-OPT-H0 and PWLR-OPT-H1 representations\n",
    "    Parallel(n_jobs,verbose=5)(delayed(data.embed_graph)(file_name, weight=\"attribute\", p=1, error=1e-12, bias=0, markov_step=30, wl_step=30, embed_type=2) for file_name in file_dataset)\n",
    "    data.concatenator(embed_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
